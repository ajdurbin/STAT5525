---
title: "Homework 1"
subtitle: 'STAT5525: Data Analytics'
author: "Alexander Durbin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE, eval = TRUE)
```


```{r, include=FALSE}

library(tidyverse)
library(MASS)
library(mvtnorm)
library(mnormt)
set.seed(1)

```

# Problem 1

## Part A

```{r, fig.cap="Simulated data from normal mixture models."}
  
# generate centroids
mu <- c(1, 0)
sigma <- diag(1, nrow = 2)
center <- mvrnorm(n = 5, mu = mu, Sigma = sigma)
centroids <- tibble(x1 = center[, 1], x2 = center[, 2], class = rep(0, 5))


mu <- c(0, 1)
center <- mvrnorm(n = 5, mu = mu, Sigma = sigma)
tmp <- tibble(x1 = center[, 1], x2 = center[, 2], class = rep(1, 5))

centroids <- bind_rows(centroids, tmp)

# generate data

my_data <- tibble(x1 = rep(0, 100), x2 = rep(0, 100), class = rep(NA, 100),
                  subclass = rep(NA, 100))

# make this a function with apply statement later
for (i in 1:nrow(my_data)) {
  
  row_index <- sample(1:10, 1)
  
  new_point <- mvrnorm(n = 1,
                       mu = c(centroids$x1[row_index], centroids$x2[row_index]),
                       Sigma = sigma / 5) 
  
  my_data$x1[i] = new_point[1]
  my_data$x2[i] = new_point[2]
  my_data$class[i] = centroids$class[row_index]
  my_data$subclass[i] = row_index
  
}

# plot data by class
ggplot(data = my_data) +
  geom_point(mapping = aes(x = x1, y = x2, shape = factor(class))) +
  theme(legend.title=element_blank()) +
  ggtitle('TRUTH') +
  ylim(-5, 5) +
  xlim(-5, 5) +
  theme_bw()


```

## Part B

```{r, fig.cap='Linear separating boundary using least squares.', warning=FALSE}

# linear model using x1, x2 to predict 0,1 response
lmfit <- lm(class ~ x1 + x2, data = my_data)

# create model matrix
tmp <- my_data %>% 
  dplyr::select(x1, x2) %>% 
  mutate(intercept = rep(1, nrow(my_data))) %>% 
  dplyr::select(intercept, x1, x2)

# predicted values
my_data$yhat <- as.matrix(tmp) %*% coefficients(lmfit)

# encoding of classes
my_data <- my_data %>% 
  mutate(pred_class_reg = ifelse(yhat > 0.5, 1, 0))

my_data <- my_data %>% 
  mutate(x1_seq = seq(min(x1), max(x1), length.out = nrow(my_data))) %>% 
  mutate(x2_seq = (0.5 - coefficients(lmfit)[1] - coefficients(lmfit)[2] * x1_seq) / coefficients(lmfit)[3])

ggplot(data = my_data) +
  geom_point(mapping = aes(x = x1, y = x2, shape = factor(class))) +
  theme(legend.title=element_blank()) +
  ggtitle('LINEAR REGRESSION BOUNDARY') +
  geom_line(mapping = aes(x = x1_seq, y = x2_seq)) +
  ylim(-5, 5) +
  xlim(-5, 5) +
  theme_bw()

```


```{r}

a <- my_data %>% 
  filter(class == 0, pred_class_reg == 0) %>% 
  nrow()

b <- my_data %>% 
  filter(class == 0, pred_class_reg == 1) %>% 
  nrow()

c <- my_data %>% 
  filter(class == 1, pred_class_reg == 0) %>% 
  nrow()

d <- my_data %>% 
  filter(class == 1, pred_class_reg == 1) %>% 
  nrow()

knitr::kable(as.data.frame(matrix(data = c(a, b, c, d),nrow = 2), row.names = c('True 0', 'True 1')), col.names = c('Predicted 0', 'Predicted 1'), caption = 'Confusion matrix for linear separating boundary using least squares.')

f_pos_reg <- b / (a + b)
f_neg_reg <- c / (c + d)

paste0('The false positive rate is ', f_pos_reg)
paste0('The false negative rate is ', f_neg_reg)

```

# Problem 2

We derive the optimal Bayes separating boundary by optimizing the posterior distribution, $Pr(G_k | X)$. Because we know how the data were generated, $Pr(x | G_k)$, we use Bayes Rule for optimization. We show $Pr(G_k = 1 | X)$ and then use Total Law of Probability to see $Pr(G_k = 2 | X) = 1 - Pr(G_k = 1 | X)$.

\begin{align*} Pr(G_k = 1| X) &= \frac{Pr(X = 1 | G_k) Pr(G_k)}{Pr(X)} \\

&= \frac{\sum_{k = 1}^{5} \pi_k N(x | m_k, \frac{I}{5}) \times \frac{1}{2}}{\sum_{k = 1}^{5} \pi_k N(x | m_k, \frac{I}{5}) \times \frac{1}{2} + \sum_{k = 6}^{10} \pi_k N(x | m_k, \frac{I}{5}) \times \frac{1}{2}} \\

&= \frac{\sum_{k = 1}^{5} \frac{1}{5} N(x | m_k, \frac{I}{5}) \times \frac{1}{2}}{\sum_{k = 1}^{5} \frac{1}{5} N(x | m_k, \frac{I}{5}) \times \frac{1}{2} + \sum_{k = 6}^{10} \frac{1}{5} N(x | m_k, \frac{I}{5}) \times \frac{1}{2}} \\ 

&= \frac{\sum_{k = 1}^{5} N(x | m_k, \frac{I}{5}) }{\sum_{k = 1}^{5} N(x | m_k, \frac{I}{5}) + \sum_{k = 6}^{10} N(x | m_k, \frac{I}{5})} \\

\end{align*}

We then assign class label based on whichever class has the larger probability.

# Problem 3

```{r, fig.cap='Bayes optimal separating boundary.'}

c1_num <- matrix(data = 0, ncol = 5, nrow = nrow(my_data))
c1_den <- matrix(data = 0, ncol = 10, nrow = nrow(my_data))

# loop through the centroids and fill each column of c1_num with the density
# for that particular centroid
for(i in 1:5){
  # c1_num[, i] <- apply(seq_mat, 1, function(g) norm_den(g, centroids[i, 1:2]))
  c1_num[, i] <- apply(
    as.matrix(my_data[, 1:2]), 1,
    function(g) dmvnorm(x = g, mean = as.matrix(centroids[i, 1:2]), sigma = diag(1/5, nrow = 2))
  )
}

# do the same except for all 10 centroids
for(i in 1:10){
  # c1_den[, i] <- apply(seq_mat, 1, function(g) norm_den(g, centroids[i, 1:2]))
  c1_den[, i] <- apply(
    as.matrix(my_data[, 1:2]), 1,
    function(g) dmvnorm(x = g, mean = as.matrix(centroids[i, 1:2]), sigma = diag(1/5, nrow = 2))
  )
}

# now sum across the rows
c1_num <- apply(c1_num, 1, sum)
c1_den <- apply(c1_den, 1, sum)

# make a dataframe and add column of the probabilities derived in class
# make a new column of 1 - class 1 probs = class 2 probs
my_data <- my_data %>%
  mutate(class_1_probs = c1_num/c1_den) %>% 
  mutate(class_2_probs = 1 - class_1_probs) %>% 
  mutate(bayes_pred_class = ifelse(class_1_probs > class_2_probs, 0, 1))




# PLOT BAYES SEPARATING BOUNDARY
# create grid of points
# grid.vector1 = seq(min(my_data$x1), max(my_data$x1), 0.1)
# grid.vector2 = seq(min(my_data$x2), max(my_data$x2), 0.1)
grid.vector1 = seq(-5, 5, by = 0.1)
grid.vector2 = seq(-5, 5, by = 0.1)
grid = expand.grid(grid.vector1, grid.vector2)

#calculate density for each point on grid for each of the multivariates distributions
prob.1 = matrix(0:0, nrow = nrow(grid), ncol = 5) #initialize grid
for (i in 1:nrow(grid)){
  for (j in 1:5){
    prob.1[i, j] = dmnorm(grid[i, ], centroids[j, 1:2], diag(1/5, nrow = 2))  
  }
}
prob1.max = apply(prob.1, 1, max)

#second class - as per above
prob.2 = matrix(0:0, nrow = nrow(grid), ncol = 5) #initialize grid
for (i in 1:nrow(grid)){
  for (j in 6:10){
    prob.2[i, j - 5] = dmnorm(grid[i, ], centroids[j, 1:2], diag(1/5, nrow = 2))  
  }
}
prob2.max = apply(prob.2, 1, max)


prob.total = cbind(prob1.max, prob2.max)
class = rep(1, times = length(prob1.max))
class[prob1.max < prob2.max] = 2
invisible(cbind(prob.total, class))


plot(grid[, 1], 
     grid[, 2],
     pch = ".", 
     cex = 2,
     col = ifelse(class == 1, "white", "white"),
     xlab = "x1",
     ylab = "x2",
     xlim = c(-5, 5),
     ylim = c(-5, 5),
     main = "Optimal Bayes Classifier Decision Boundary"
    )


values1 <- my_data %>% 
  filter(class == 0)

values2 <- my_data %>% 
  filter(class == 1)

points(values1, pch = 16)
points(values2, pch = 17) 

prob.bayes <- matrix(prob1.max / (prob1.max + prob2.max),
                     length(grid.vector1),
                     length(grid.vector2)
                    )

contour(x = grid.vector1, 
        y = grid.vector2,
        z = prob.bayes, 
        levels = 0.5, 
        labels = "", 
        lwd = 1,
        add = TRUE
        )

```

```{r}

a <- my_data %>% 
  filter(class == 0, bayes_pred_class == 0) %>% 
  nrow()

b <- my_data %>% 
  filter(class == 0, bayes_pred_class == 1) %>% 
  nrow()

c <- my_data %>% 
  filter(class == 1, bayes_pred_class == 0) %>% 
  nrow()

d <- my_data %>% 
  filter(class == 1, bayes_pred_class == 1) %>% 
  nrow()

f_pos_bayes <- b / (a + b)
f_neg_bayes <- c / (c + d)

knitr::kable(as.data.frame(matrix(data = c(a, b, c, d),nrow = 2), row.names = c('True 0', 'True 1')), col.names = c('Predicted 0', 'Predicted 1'), caption = 'Confusion matrix for optimal Bayes separating boundary.')

paste0('The false positive rate is ', f_pos_bayes)
paste0('The false negative rate is ', f_neg_bayes)

```

# Problem 4

Having previous knowledge of the superclass labels and knowing that there are five subclasses for each superclass, we use *k*-means to estimate the five means for each superclass and then perform the Bayes optimal classification procedure outlined in Problem 2 and Problem 3.

```{r}

class_1 <- my_data %>% 
  filter(class == 0)

class_2 <- my_data %>% 
  filter(class == 1)

kclass1 <- kmeans(class_1[, 1:2], centers = 5)
kclass2 <- kmeans(class_2[, 1:2], centers = 5)

k_centroids1 <- tibble(x1 = kclass1$centers[, 1], x2 = kclass1$centers[, 2], class = rep(0, 5))
k_centroids2 <- tibble(x1 = kclass2$centers[, 1], x2 = kclass2$centers[, 2], class = rep(1, 5))
k_centroids <- rbind(k_centroids1, k_centroids2)

c1_num <- matrix(data = 0, ncol = 5, nrow = nrow(my_data))
c1_den <- matrix(data = 0, ncol = 10, nrow = nrow(my_data))

# loop through the centroids and fill each column of c1_num with the density
# for that particular centroid
for(i in 1:5){
  # c1_num[, i] <- apply(seq_mat, 1, function(g) norm_den(g, centroids[i, 1:2]))
  c1_num[, i] <- apply(
    as.matrix(my_data[, 1:2]), 1,
    function(g) dmvnorm(x = g, mean = as.matrix(k_centroids[i, 1:2]), sigma = diag(1/5, nrow = 2))
  )
}

# do the same except for all 10 centroids
for(i in 1:10){
  # c1_den[, i] <- apply(seq_mat, 1, function(g) norm_den(g, centroids[i, 1:2]))
  c1_den[, i] <- apply(
    as.matrix(my_data[, 1:2]), 1,
    function(g) dmvnorm(x = g, mean = as.matrix(k_centroids[i, 1:2]), sigma = diag(1/5, nrow = 2))
  )
}

# now sum across the rows
c1_num <- apply(c1_num, 1, sum)
c1_den <- apply(c1_den, 1, sum)

# make a dataframe and add column of the probabilities derived in class
# make a new column of 1 - class 1 probs = class 2 probs
my_data <- my_data %>%
  mutate(kclass_1_probs = c1_num/c1_den) %>% 
  mutate(kclass_2_probs = 1 - class_1_probs) %>% 
  mutate(kbayes_pred_class = ifelse(class_1_probs > class_2_probs, 0, 1))

# create grid of points
# grid.vector1 = seq(min(my_data$x1), max(my_data$x1), 0.1)
# grid.vector2 = seq(min(my_data$x2), max(my_data$x2), 0.1)
grid.vector1 = seq(-5, 5, by = 0.1)
grid.vector2 = seq(-5, 5, by = 0.1)
grid = expand.grid(grid.vector1, grid.vector2)

#calculate density for each point on grid for each of the multivariates distributions
kprob.1 = matrix(0:0, nrow = nrow(grid), ncol = 5) #initialize grid
for (i in 1:nrow(grid)){
  for (j in 1:5){
    kprob.1[i, j] = dmnorm(grid[i, ], k_centroids[j, 1:2], diag(1/5, nrow = 2))  
  }
}
kprob1.max = apply(kprob.1, 1, max)

#second class - as per above
kprob.2 = matrix(0:0, nrow = nrow(grid), ncol = 5) #initialize grid
for (i in 1:nrow(grid)){
  for (j in 6:10){
    kprob.2[i, j - 5] = dmnorm(grid[i, ], k_centroids[j, 1:2], diag(1/5, nrow = 2))  
  }
}
kprob2.max = apply(kprob.2, 1, max)


kprob.total = cbind(kprob1.max, kprob2.max)
class = rep(1, times = length(kprob1.max))
class[kprob1.max < kprob2.max] = 2
invisible(cbind(kprob.total, class))

plot(grid[, 1], 
     grid[, 2],
     pch = ".", 
     cex = 2,
     col = ifelse(class == 1, "white", "white"),
     xlab = "x1",
     ylab = "x2",
     xlim = c(-5, 5),
     ylim = c(-5, 5),
     main = "k-Means Optimal Bayes Classifier Decision Boundary"
    )


values1 <- my_data %>% 
  filter(class == 0)

values2 <- my_data %>% 
  filter(class == 1)

points(values1, pch = 16)
points(values2, pch = 17) 

kprob.bayes <- matrix(kprob1.max / (kprob1.max + kprob2.max),
                     length(grid.vector1),
                     length(grid.vector2)
                    )

contour(x = grid.vector1, 
        y = grid.vector2,
        z = kprob.bayes, 
        levels = 0.5, 
        labels = "", 
        lwd = 1,
        add = TRUE
        )


```

```{r}

a <- my_data %>% 
  filter(class == 0, kbayes_pred_class == 0) %>% 
  nrow()

b <- my_data %>% 
  filter(class == 0, kbayes_pred_class == 1) %>% 
  nrow()

c <- my_data %>% 
  filter(class == 1, kbayes_pred_class == 0) %>% 
  nrow()

d <- my_data %>% 
  filter(class == 1, kbayes_pred_class == 1) %>% 
  nrow()

k_f_pos_bayes <- b / (a + b)
k_f_neg_bayes <- c / (c + d)

knitr::kable(as.data.frame(matrix(data = c(a, b, c, d),nrow = 2), row.names = c('True 0', 'True 1')), col.names = c('Predicted 0', 'Predicted 1'), caption = 'Confusion matrix for optimal Bayes separating boundary using k-means to approximate the subclass means.')

paste0('The false positive rate is ', k_f_pos_bayes)
paste0('The false negative rate is ', k_f_neg_bayes)

```

We see that the error rates for this method are exactly the same as Problem 4. So that as long as we know how many subclasses there are for each cluster, we do not need to actually know the subclass means when performing optimal Bayes classification.

# Problem 5

If we do not know the subclass labels, we should not use the approach outlined in PRoblem 4. Instead, we can initially estimate the two superclass means using *k*-means and then assign class labels using *EM* algorithm.

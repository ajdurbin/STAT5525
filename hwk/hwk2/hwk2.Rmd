---
title: "Homework 2"
subtitle: "STAT5525: Data Analytics"
author: "Alexander Durbin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

```{r, include=FALSE}
# for logistic regression and cross validation
library(caret)
# for logistic lasso
library(glmnet)
# for classification tree
library(rpart)
```

# Part 1
## Part a
## Part b

Let $A$ be the invertible matrix that standardizes the data in $X$. Also let $Z=A\theta$ and $\beta = A\theta$. We want to show that our objective functions are not equivalent.

The LASSO solution is
\[ \hat{\beta} = \min_\beta ||Y-X\beta||_2 + \lambda^T|\beta|.\]

Our standardized solution is
\begin{align*}
\hat{\theta} &= \min_\theta ||Y-Z\theta||_2 + \lambda^T|\theta| \\
&= \min_\theta ||Y-XAA^{-1}\beta||_2 + \lambda^T|A^{-1}\beta| \\
&= \min_\theta ||Y-X\beta||_2 + \lambda^T|A^{-1}\beta| \\
&\ne \min_\beta ||Y-X\beta||_2 + \lambda^T|\beta|.
\end{align*}

Since the objective functions are different, the LASSO solutions should also be different. Hence LASSO is not invariant to standardization of data.


# Part 2
It suffices to solve this problem in two steps. We first show that the Lagrangian and constrained LASSO definitions are equivalent. We then show the correspondence between the $\beta_k$'s and $\beta_k^c$'s.

We rewrite \[\min_\beta ||Y-X\beta||_2\ subject\ to\ \sum|\beta_k| < s,\]
with a Lagrange multiplier as \[\min_\beta ||Y-X\beta||_2 + \lambda(\sum|\beta_k| - s). \]

The above is then equivalent to
\[\min_\beta ||Y-X\beta||_2 + \lambda\sum|\beta_k| - \lambda s. \]

Since $\lambda s$ does not depend on $\beta$, the previous line is equivalent to
\[\min_\beta ||Y-X\beta||_2 + \lambda\sum|\beta_k| \] when we minimize with respect to $\beta$, the lagrangian form of the LASSO.

Now to show correspondence between $\beta_k$'s and $\beta_k^c$'s.


\begin{align*}
\min_\beta ||Y-X\beta||_2 + \lambda \sum_{j=1}^p |\beta_j| &= \min_\beta \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j|  \\
&= \min_\beta \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \bar{x}_{j}\beta_j + \sum_{j=1}^p \bar{x}_{j}\beta_j \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j| \\
&= \min_\beta \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \bar{x}_{j}\beta_j -  \sum_{j=1}^p (x_{ij} - \bar{x}_j)\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j| \\
&= \min_\beta \sum_{i=1}^n (y_i - \beta^c_0 - \sum_{j=1}^p x^c_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j| \\
&= \min_\beta ||Y-X\beta^c||_2 + \lambda \sum_{j=1}^p |\beta_j^c|.
\end{align*}

So that for $j = 1, 2, ..., p$, $\beta_j = \beta_j^c$. But for the intercept term, $\beta_0^c = \beta_0 - \sum_{j=1}^p \bar{x}_j\beta_j$.

# Part 3
## Part a

```{r}

set.seed(130)
# load spam data set
raw <- read.table("spam.data.txt")
# standardize columns
raw[, 1 : 57] <- scale(raw[, 1 : 57])
# factor the response
raw[, 58] <- as.factor(raw[, 58])

```

## Part b

The following code constructs a classification tree.

```{r}

# trc <- trainControl(method = "cv", number = 10)
# carfit <- train(V58 ~ ., data = raw, trControl = trc, method = "rpart", 
#                 control = list(maxdepth = 1000, minsplit = 2))
# carfit
# carfit$finalModel
# carfit$resample
```

# Part 4

The following code performs logistic regression with 10 cross fold validation and provides overall accuracy rates for each fold.

```{r, warning=FALSE}

trc <- trainControl(method = "cv", number = 10)
logfit <- train(V58 ~ ., data = raw, trControl = trc, method = "glm",
               family = binomial())  
logfit$resample

```
Our individual error rates are the following.

```{r}

1 - logfit$resample[, 1]

```

And the overall error rate is the following.

```{r}

1 - logfit$results[, 2]

```

# Part 5

The following code fits the logistic LASSO. We then extract the Lambda value with the minimum deviance from our cross validation runs. We use this Lambda to then make predictions. The resulting overall error rate is the following.

```{r}

x <- as.matrix(raw[, 1 : 57])
y <- raw[, 58]

lassofit <- cv.glmnet(x = as.matrix(x), y = y, nfolds = 10, type.measure = "deviance", alpha = 1, family = "binomial")

preds <- predict(lassofit, as.matrix(x), s = "lambda.min", type = "class")

paste0(" Overall error rate: ", 1 - sum(y == preds) / nrow(raw))

```

So the logistic LASSO does perform better than logistic regression.
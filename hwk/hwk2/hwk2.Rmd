---
title: "Homework 2"
subtitle: "STAT5525: Data Analytics"
author: "Alexander Durbin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# for logistic regression and cross validation
library(caret)
# for logistic lasso
library(glmnet)
```

```{r}

set.seed(130)
# load spam data set
raw <- read.table("spam.data.txt")
# standardize columns
raw[, 1 : 57] <- scale(raw[, 1 : 57])
# factor the response
raw[, 58] <- as.factor(raw[, 58])

```
# Part 4

The following code performs logistic regression with 10 cross fold validation and provides overall accuracy rates for each fold.

```{r, warning=FALSE}

trc <- trainControl(method = "cv", number = 10)
logfit <- train(V58 ~ ., data = raw, trControl = trc, method = "glm",
               family = binomial())  
logfit$resample

```
Our individual error rates are the following.

```{r}

1 - logfit$resample[, 1]

```

And the overall error rate is the following.

```{r}

1 - logfit$results[, 2]

```

# Part 5

The following code fits the logistic LASSO. We then extract the Lambda value with the minimum deviance from our cross validation runs. We use this Lambda to then make predictions. The resulting overall error rate is the following.

```{r}

x <- as.matrix(raw[, 1 : 57])
y <- raw[, 58]

lassofit <- cv.glmnet(x = as.matrix(x), y = y, nfolds = 10, type.measure = "deviance", alpha = 1, family = "binomial")

preds <- predict(lassofit, as.matrix(x), s = "lambda.min", type = "class")

paste0(" Overall error rate: ", 1 - sum(y == preds) / nrow(raw))

```

So the logistic LASSO does perform better than logistic regression.